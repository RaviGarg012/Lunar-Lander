{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059246a2-77ed-45b4-8b93-e161c005dfdc",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666834c0-8fc0-45c9-87cb-97fb52b6ce5d",
   "metadata": {},
   "source": [
    "## Lunar lander"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d906095d-9a93-4bbe-a5c1-1b2cde8c311c",
   "metadata": {},
   "source": [
    "#### Liberaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc661d1e-a31b-45f5-a276-3a780a8157b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import LunarUtils\n",
    "# deque is our data structure for our memory buffer\n",
    "from collections import deque, namedtuple\n",
    "# namedtuple used to store experience tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9f2ae34-7d7e-41a6-af94-0c53c2518bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "# gym is environment liberary used in reinforcement to test the reinforcement learning algorithm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d953b7fb-0fd6-41de-803a-09364ae80cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIL and pyvirtualdisplay used to render the lunar lander environment\n",
    "from PIL import Image\n",
    "from pyvirtualdisplay import Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24ad8a76-7062-4c0e-98fa-95879ce11f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow liberaries for the neural network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c84b6dd0-03b5-4f40-8637-ced814032426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the virtual display for lunar lander environment\n",
    "# Display(visible=0, size=(840, 480)).start();\n",
    "# set the random seed for tensorflow\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d88489ef-a5ef-4280-ac1e-1136de3b7ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hyperparameter values used for reinforcement environment\n",
    "MEMORY_SIZE = 100_000 # size of memory buffer\n",
    "GAMMA = 0.995 # discount factor\n",
    "ALPHA = 1e-3 # learning rate\n",
    "TAU = 1e-3 # soft update\n",
    "NUM_STEP_FOR_UPDATE = 4 # perform a learning step after c time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7770988a-235c-4743-89d8-dedf92638569",
   "metadata": {},
   "source": [
    "#### Lunar lander Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bd10de-bca3-4ef6-9c3d-482b3d7ecb8b",
   "metadata": {},
   "source": [
    "> lunar lander can do four actions : 0. do nothing, 1. fire left engine 2. fire main engine 3. fire right engine\n",
    "\n",
    "> the environment considered solved if the lunar lander gets 200 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdddd019-2ce2-49e1-943a-eeb4e98547d9",
   "metadata": {},
   "source": [
    "#### Observation space\n",
    "\n",
    "agent's observation space contains 8 parameters\n",
    "\n",
    "* (x, y) coordinates in space\n",
    "* $(\\dot x, \\dot y)$ velocity in x and y direction\n",
    "* $\\theta$ angle\n",
    "* $\\dot \\theta$ angular velocity\n",
    "* l and r, which represent that each leg of lunar lender is on the groud or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c8e8820-438e-4b4e-b2a2-fd75075310e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the environment\n",
    "env = gym.make('LunarLander-v3',  render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe0053b0-f8cc-42b5-b498-68bcd4e531ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=600x400>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reset the environment\n",
    "env.reset()\n",
    "# render the first image of the environment\n",
    "Image.fromarray(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "142cc4f6-410c-4b9f-af08-ad2c7fedbd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of state vector:  (8,)\n",
      "no. of actions:  4\n"
     ]
    }
   ],
   "source": [
    "# now get the size of state vector and no. of actions\n",
    "state_size = env.observation_space.shape\n",
    "num_action = env.action_space.n\n",
    "print(\"size of state vector: \", state_size)\n",
    "print(\"no. of actions: \", num_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcb61d9-2f2e-4aa2-a80d-6199585886e9",
   "metadata": {},
   "source": [
    "#### Interacting with gym environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778daf25-8dc0-4240-8345-b2e39a656d2f",
   "metadata": {},
   "source": [
    " `.step()` method in gym liberary take action and return four values\n",
    "\n",
    "* observation : a numpy array containing velocities and position of lunar lander\n",
    "* reward : amount of reward return as result after taking an action\n",
    "* done(boolean) : done return a boolean value if its true then it show that current episode is terminated, reset the environment\n",
    "* info : information use for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8565ee5-ac29-4d7c-be82-4ae56b128bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment to get the current state\n",
    "current_state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76ba6de3-0ecd-415e-a259-4708f26a6d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current state :  (array([ 0.00204124,  1.4213156 ,  0.20674613,  0.46202615, -0.00235855,\n",
      "       -0.046831  ,  0.        ,  0.        ], dtype=float32), {})\n",
      "next state :  [ 0.00408258  1.4311326   0.2064666   0.43630913 -0.00467255 -0.04628339\n",
      "  0.          0.        ]\n",
      "reward :  1.134400110817694\n",
      "done :  False\n"
     ]
    }
   ],
   "source": [
    "# select an action\n",
    "action = 0 # do nothing\n",
    "\n",
    "# on give action run the step function\n",
    "next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "# print the obseravation\n",
    "print(\"current state : \", current_state)\n",
    "print(\"next state : \", next_state)\n",
    "print(\"reward : \", reward)\n",
    "print(\"done : \", done)\n",
    "\n",
    "# assign next_state to current state\n",
    "current_state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fd708b-0d2f-4f28-afa9-6a1152fad418",
   "metadata": {},
   "source": [
    "### Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb33ab0-cb71-449d-8fac-3d71afd71c96",
   "metadata": {},
   "source": [
    "using neural network for the reinforcement learning is highly unstable, but using some technique the unstability can be avoided, these techniques are\n",
    "\n",
    "1. Target Network\n",
    "2. Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8ad566-d234-463b-a914-dd4fb98b7570",
   "metadata": {},
   "source": [
    "#### 1. Target Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9052ced6-1008-40eb-93e7-3a368e0c0b9a",
   "metadata": {},
   "source": [
    "the target y is variable at every iteration, so we need two neural network one for target, and one for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed5f76a7-e709-4733-b529-78f45771afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-network\n",
    "q_network = Sequential([\n",
    "    # input layer\n",
    "    Input(shape=(state_size)),\n",
    "    # first hidden layer\n",
    "    Dense(units=64, activation='relu'),\n",
    "    # second hidden layer\n",
    "    Dense(units=64, activation='relu'),\n",
    "    # output layer\n",
    "    Dense(units=num_action, activation='linear')\n",
    "])\n",
    "\n",
    "# target-Q-network\n",
    "target_q_network = Sequential([\n",
    "    # input layer\n",
    "    Input(shape=(state_size)),\n",
    "    # first hidden layer\n",
    "    Dense(units=64, activation='relu'),\n",
    "    # second hidden layer\n",
    "    Dense(units=64, activation='relu'),\n",
    "    # output layer\n",
    "    Dense(units=num_action, activation='linear')\n",
    "])\n",
    "\n",
    "# cost\n",
    "optimizer = Adam(learning_rate=ALPHA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa69f54-38ee-412b-bba0-43380762c717",
   "metadata": {},
   "source": [
    "#### 2. Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334c7cb5-8f0c-4943-a3b7-245c4af256dd",
   "metadata": {},
   "source": [
    "in experience replay, we store previous obeservations like $(S_t, A_t, R_t, S_{t+1})$ in the memory buffer, after we select the random min batch of experience from the buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24df991f-a49c-4598-9637-e533f90878eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the experience as name tuple\n",
    "Experience = namedtuple(\"Experience\", field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fbdf1c-a818-4f14-88a7-5f244e4336a3",
   "metadata": {},
   "source": [
    "### Deep Q-Learning with Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25152e84-d190-4fd7-99ec-134cba1f1a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first compute the loss between y_target and q_value\n",
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    '''\n",
    "    # inputs\n",
    "    experiences : namedtuple that contains the experience replay\n",
    "    gamma : discount factor\n",
    "    q_network : keras model to generate q_values\n",
    "    target_q_network : keras model to generate y_target\n",
    "\n",
    "    # output\n",
    "    loss : mean squared error between q_values and y_target\n",
    "    '''\n",
    "\n",
    "    # unpack min batch of experience tuple\n",
    "    states, actions, rewards, next_states, done_vals = experiences\n",
    "\n",
    "    # comput max Q(s, a)\n",
    "    max_qsa = tf.reduce_max(target_q_network(next_states), axis = -1)\n",
    "\n",
    "    # find the y target value\n",
    "    y_target = rewards + (1 - done_vals)*(max_qsa*gamma)\n",
    "\n",
    "    # get the q_values and reshape to match the y_target\n",
    "    q_values = q_network(states)\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "                                                tf.cast(actions, tf.int32)], axis=1))\n",
    "\n",
    "    # compute loss\n",
    "    loss = MSE(q_values, y_target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a23504-c9c3-447b-ae2f-c2acc272b176",
   "metadata": {},
   "source": [
    "#### update the network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80e22274-60fa-4631-b7d4-b7ef3e64c720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent learn method update the weights of q_network and target_q_network using custom training loop\n",
    "@tf.function\n",
    "def agent_learn(experience, gamma):\n",
    "    # update the weights of Q network\n",
    "    # calculate the loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(experience, gamma, q_network, target_q_network)\n",
    "\n",
    "    # get the gradient of the loss w.r.t the weights\n",
    "    gradient = tape.gradient(loss, q_network.trainable_variables)\n",
    "\n",
    "    # update the weight of q_network\n",
    "    optimizer.apply_gradients(zip(gradient, q_network.trainable_variables))\n",
    "\n",
    "    # update the weight of target_q_network\n",
    "    for target_weights, q_net_weights in zip(target_q_network.weights, q_network.weights):\n",
    "        target_weights.assign(TAU * q_net_weights + (1.0 - TAU) * target_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53585f9-f70f-42fd-beac-c57e655959ae",
   "metadata": {},
   "source": [
    "### Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0675f1c2-8d52-4efb-8c24-ba1cedf86800",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 | Total point average of the last 100 episodes: -166.78\n",
      "Episode 200 | Total point average of the last 100 episodes: -93.311\n",
      "Episode 300 | Total point average of the last 100 episodes: -29.69\n",
      "Episode 400 | Total point average of the last 100 episodes: 76.585\n",
      "Episode 500 | Total point average of the last 100 episodes: 134.78\n",
      "Episode 600 | Total point average of the last 100 episodes: 185.37\n",
      "Episode 630 | Total point average of the last 100 episodes: 198.47"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 631 | Total point average of the last 100 episodes: 201.23\n",
      "\n",
      "Environment solved in 631 episodes!\n",
      "\n",
      "Total Runtime: 1720.22 s (28.67 min)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# number of episodes\n",
    "num_episodes = 2000\n",
    "# max number of steps a agent can take during an episode\n",
    "max_num_timestamps = 1000\n",
    "# store the history of points rewards\n",
    "total_point_history = []\n",
    "\n",
    "# min batch size\n",
    "MINIBATCH_SIZE = 64\n",
    "# number of total points use for averaging\n",
    "num_p_av = 100\n",
    "# initialize Epsilon for E-greedy policy\n",
    "epsilon = 1.0\n",
    "\n",
    "# create  a memory buffer\n",
    "memory_buffer = deque(maxlen = MEMORY_SIZE)\n",
    "\n",
    "# set the target_q_network weights\n",
    "target_q_network.set_weights(q_network.get_weights())\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    # reset the environment state\n",
    "    state = env.reset()[0]\n",
    "    total_points = 0\n",
    "\n",
    "    for t in range(max_num_timestamps):\n",
    "        # from the current state S choose the action A using e greedy policy\n",
    "        state_q = np.expand_dims(state, axis = 0)\n",
    "        q_values = q_network(state_q)\n",
    "        if random.random() > epsilon:\n",
    "            action = np.argmax(q_values.numpy()[0])\n",
    "        else:\n",
    "            action = random.choice(list(np.arange(4)))\n",
    "\n",
    "        # step method to get the observation\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        # store the experience tuple in the memory_buffer\n",
    "        memory_buffer.append(Experience(state, action, reward, next_state, done))\n",
    "\n",
    "        # update the nework\n",
    "        if (t + 1) % NUM_STEP_FOR_UPDATE == 0 and len(memory_buffer) > MINIBATCH_SIZE:\n",
    "            # sample random mini batch experience from memory_buffer\n",
    "            experience = LunarUtils.get_experiences(memory_buffer)\n",
    "            # set the y_target and update the weights\n",
    "            agent_learn(experience, GAMMA)\n",
    "\n",
    "        # copy the next state to current\n",
    "        state = next_state.copy()\n",
    "        total_points += reward\n",
    "        \n",
    "        # Live render every 100 episodes\n",
    "        if i % 100 == 0:\n",
    "            env.render()\n",
    "            time.sleep(0.01)  # Adjust speed of rendering\n",
    "            \n",
    "        # if done is true then break\n",
    "        if done:\n",
    "            break\n",
    "    total_point_history.append(total_points)\n",
    "    av_latest_points = np.mean(total_point_history[-num_p_av:])\n",
    "    \n",
    "    # Update the ε value\n",
    "    epsilon = LunarUtils.get_new_eps(epsilon)\n",
    "\n",
    "    print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\", end=\"\")\n",
    "\n",
    "    if (i+1) % num_p_av == 0:\n",
    "        print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\")\n",
    "\n",
    "    # consider that the environment is solved if we get an\n",
    "    # average of 200 points in the last 100 episodes.\n",
    "    if av_latest_points >= 200.0:\n",
    "        print(f\"\\n\\nEnvironment solved in {i+1} episodes!\")\n",
    "        q_network.save('lunar_lander_model.h5')\n",
    "        break\n",
    "        \n",
    "tot_time = time.time() - start\n",
    "\n",
    "print(f\"\\nTotal Runtime: {tot_time:.2f} s ({(tot_time/60):.2f} min)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2994245a-2dc0-49a7-9460-0c909c5494d2",
   "metadata": {},
   "source": [
    "### See the trained Agent In action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7dce4578-ae2f-478e-b53f-d608790b7242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished with Total Reward: 246.03\n",
      "Episode 2 finished with Total Reward: 66.74\n",
      "Episode 3 finished with Total Reward: -2.68\n",
      "Episode 4 finished with Total Reward: 1.59\n",
      "Episode 5 finished with Total Reward: 0.90\n",
      "Episode 6 finished with Total Reward: -0.64\n",
      "Episode 7 finished with Total Reward: -0.17\n",
      "Episode 8 finished with Total Reward: 0.98\n",
      "Episode 9 finished with Total Reward: -0.35\n",
      "Episode 10 finished with Total Reward: -1.60\n",
      "Episode 11 finished with Total Reward: 0.15\n",
      "Episode 12 finished with Total Reward: 0.16\n",
      "Episode 13 finished with Total Reward: -0.02\n",
      "Episode 14 finished with Total Reward: -1.88\n",
      "Episode 15 finished with Total Reward: 1.55\n",
      "Episode 16 finished with Total Reward: 0.89\n",
      "Episode 17 finished with Total Reward: 1.19\n",
      "Episode 18 finished with Total Reward: -0.22\n",
      "Episode 19 finished with Total Reward: -0.89\n",
      "Episode 20 finished with Total Reward: -1.12\n"
     ]
    }
   ],
   "source": [
    "# using open cv to rendering the agent interacting with environment\n",
    "import cv2\n",
    "\n",
    "# Create the Lunar Lander environment\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "\n",
    "num_episodes = 20  # Change this to run more episodes\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        state_q = np.expand_dims(state, axis = 0)\n",
    "        q_values = q_network(state_q)\n",
    "        action = np.argmax(q_values.numpy()[0]) \n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # Render the frame\n",
    "        frame = env.render()\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # Convert RGB to BGR (OpenCV format)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow(\"Lunar Lander - performance\", frame)\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"q\"):  # Press 'q' to quit\n",
    "            break\n",
    "\n",
    "    print(f\"Episode {episode + 1} finished with Total Reward: {total_reward:.2f}\")\n",
    "    time.sleep(1)  # Pause before the next episode\n",
    "\n",
    "# Close the OpenCV window and environment\n",
    "env.close()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dc7b56-3f9a-4bce-aee4-8494d18b3644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
